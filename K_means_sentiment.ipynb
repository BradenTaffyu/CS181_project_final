{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 unlabeled samples from aclImdb\\train\\unsup\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i admit the great majority of films released b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>take a low budget inexperienced actors doublin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>everybody has seen back to the future right wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doris day was an icon of beauty in singing and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>after a series of silly funloving movies was a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  i admit the great majority of films released b...\n",
       "1  take a low budget inexperienced actors doublin...\n",
       "2  everybody has seen back to the future right wh...\n",
       "3  doris day was an icon of beauty in singing and...\n",
       "4  after a series of silly funloving movies was a..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text\n",
    "\n",
    "\n",
    "unsup_dir = Path('aclImdb') / 'train' / 'unsup'\n",
    "if not unsup_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory not found: {unsup_dir}\")\n",
    "unsup_texts = []\n",
    "for file_path in unsup_dir.glob('*.txt'):\n",
    "    raw = file_path.read_text(encoding='utf-8')\n",
    "    unsup_texts.append(preprocess_text(raw))\n",
    "\n",
    "print(f\"Loaded {len(unsup_texts)} unlabeled samples from {unsup_dir}\")\n",
    "\n",
    "df_unsup = pd.DataFrame({'text': unsup_texts})\n",
    "df_unsup.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (50000, 151301)\n",
      "SVD explained variance (100 components): 12.31%\n",
      "Preprocessed dataset shape: (50000, 500)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "\n",
    "custom_stop = set(ENGLISH_STOP_WORDS)\n",
    "custom_stop.discard('not')\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=list(custom_stop),\n",
    "    max_df=0.8,\n",
    "    min_df=5,\n",
    "    ngram_range=(1,2),    \n",
    "    sublinear_tf=True,    \n",
    "    norm='l2'             \n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(df_unsup['text'])\n",
    "print(f\"TF-IDF matrix shape: {X_tfidf.shape}\")\n",
    "\n",
    "# SVD\n",
    "svd = TruncatedSVD(n_components=500, random_state=24)\n",
    "X_svd = svd.fit_transform(X_tfidf)\n",
    "print(f\"SVD explained variance (100 components): {svd.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_svd)\n",
    "print(f\"Preprocessed dataset shape: {X.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m kmeans_models \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 2\u001b[0m terms \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      4\u001b[0m X_vis \u001b[38;5;241m=\u001b[39m X_svd[:, :\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "kmeans_models = {}\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "X_vis = X_svd[:, :2]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, k in zip(axes, [2, 3, 5]):\n",
    "\n",
    "    km = KMeans(n_clusters=k, random_state=24, n_init=10)\n",
    "    km.fit(X_tfidf)\n",
    "    kmeans_models[k] = km\n",
    "\n",
    "    # Print top 5 terms per cluster\n",
    "    print(f\"\\nTop 5 tesrms per cluster for k={k}:\")\n",
    "    order = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    for i in range(k):\n",
    "        top_terms = [terms[idx] for idx in order[i, :5]]\n",
    "        print(f\" Cluster {i}: {', '.join(top_terms)}\")\n",
    "\n",
    "\n",
    "    # Visualize clusters in 2D\n",
    "    ax.scatter(X_vis[:, 0], X_vis[:, 1], c=km.labels_, cmap='tab10', alpha=0.6)\n",
    "    ax.set_title(f'KMeans Clusters (k={k})')\n",
    "    ax.set_xlabel('SVD Component 1')\n",
    "    ax.set_ylabel('SVD Component 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of data points per cluster for k=2:\n",
      " Cluster 0: 28075\n",
      " Cluster 1: 21925\n",
      "\n",
      "Number of data points per cluster for k=3:\n",
      " Cluster 0: 21723\n",
      " Cluster 1: 12471\n",
      " Cluster 2: 15806\n",
      "\n",
      "Number of data points per cluster for k=5:\n",
      " Cluster 0: 1952\n",
      " Cluster 1: 17405\n",
      " Cluster 2: 7966\n",
      " Cluster 3: 10446\n",
      " Cluster 4: 12231\n"
     ]
    }
   ],
   "source": [
    "for k, km in sorted(kmeans_models.items()):\n",
    "    labels = km.labels_\n",
    "    counts = [(labels == i).sum() for i in range(k)]\n",
    "    print(f\"\\nNumber of data points per cluster for k={k}:\")\n",
    "    for idx, count in enumerate(counts):\n",
    "        print(f\" Cluster {idx}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved KMeans model for k=2 to kmeans_model_k2.pkl\n",
      "Saved KMeans model for k=3 to kmeans_model_k3.pkl\n",
      "Saved KMeans model for k=5 to kmeans_model_k5.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "for k, km in kmeans_models.items():\n",
    "    filename = f'kmeans_model_k{k}.pkl'\n",
    "    joblib.dump(km, filename)\n",
    "    print(f\"Saved KMeans model for k={k} to {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
